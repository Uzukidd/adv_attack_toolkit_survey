# Summary

## ![img](summary.assets/Trusted-AI.png)[Trusted-AI/*adversarial-robustness-toolbox*](https://github.com/Trusted-AI/adversarial-robustness-toolbox)

- Org: LF AI & Data (Linux Foundation)
- Desc: Adversarial Robustness Toolbox (ART) - Python Library for Machine Learning Security - Evasion, Poisoning, Extraction, Inference - Red and Blue Teams
- Latest update: Aug 25, 2023
- Framework: TensorFlow，Keras，PyTorch，MXNet，scikit-learn，XGBoost，LightGBM，CatBoost，GPy etc.
- Target: images, tables, audio, video, etc / classification, object detection, speech recognition, generation, certification, etc

## ![img](summary.assets/advboxes.png)[advboxes/AdvBox](https://github.com/advboxes/AdvBox)

- Org: Baidu X-Lab
- Desc: Advbox is a toolbox to generate adversarial examples that fool neural networks in PaddlePaddle、PyTorch、Caffe2、MxNet、Keras、TensorFlow and Advbox can benchmark the robustness of machine learning models. Advbox give a command line tool to generate adversarial examples with Zero-Coding.
- Latest update: Aug 8, 2022
- Framework: PaddlePaddle, Caffe, Keras, PyTorch, MxNet
- Target: 2d-image classification
- Attacks implemented:
  - White-box
    - L-BFGS
    - FGSM
    - BIM
    - ILCM
    - MI-FGSM
    - JSMA
    - DeepFool
    - C/W

  - Black-box
    - Single Pixel Attack
    - Local Search Attack

- Defense implemented:
  - Feature Fqueezing
  - Spatial Smoothing
  - Label Smoothing
  - Gaussian Augmentation
  - Adversarial Training


## ![img](summary.assets/BorealisAI.png)[BorealisAI/advertorch](https://github.com/BorealisAI/advertorch)

- Org: Borealis AI (Royal Bank of Canada.)
- Desc: [![advertorch text](summary.assets/advertorch.png)](https://github.com/borealisai/advertorch) is a Python toolbox for adversarial robustness research. The primary functionalities are implemented in PyTorch. Specifically, AdverTorch contains modules for generating adversarial perturbations and defending against adversarial examples, also scripts for adversarial training.
- Latest update: May 30, 2022
- Framework: PyTorch
- 

## ![Owner avatar](summary.assets/1492758.png)[IntelLabs/MART](https://github.com/IntelLabs/MART)

- Org: Intel Labs (Intel)
- Desc: **Modular Adversarial Robustness Toolkit** makes it easy to compose novel attacks to evaluate adversarial robustness of deep learning models. Thanks to the modular design of the optimization-based attack framework, you can use off-the-shelf elements, such as optimizers and learning rate schedulers, from PyTorch to compose powerful attacks. The unified framework also supports advanced features, such as early stopping, to improve attack efficiency.
- Latest update: Aug 25, 2023
- Framework: Tensorflow